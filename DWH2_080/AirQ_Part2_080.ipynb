{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea63db15",
   "metadata": {},
   "source": [
    "\n",
    "# **AirQ_Part2_xxx** — Assignment 2 (ETL + OLAP with Atoti)\n",
    "\n",
    "- This notebook orchestrates Assignment 2.\n",
    "- All SQL must live in external `.sql` files under `ddl/`, `etl/`, and `sql/`. \n",
    "- All MDX must live in external `.mdx` files under `mdx/`.\n",
    "\n",
    "**Final folder layout (per‑group, self‑contained)**\n",
    "\n",
    "```\n",
    "BI_Projects/\n",
    "  DWH2_xxx/\n",
    "    csv/       # 15 OLTP CSV files\n",
    "    ddl/       # DDL only (staging, warehouse)\n",
    "    etl/       # ETL steps: a2_etl*.sql files\n",
    "    mdx/       # MDX-queries in .mdx files: a2_q{NN}_{A|B}.mdx\n",
    "    mdx_out/   # CSV files with the results of MDX-queries\n",
    "    pdf/       # PDF files with dashboard exports: a2_q{NN}.pdf\n",
    "    sql/       # SQL-queries in .sql files: a2_q{NN}_{A|B}.sql\n",
    "    sqldump/   # Export produced by pg_dump\n",
    "    AirQ_Part2_xxx.ipynb\n",
    "    group_xxx.txt\n",
    "    Report_Part2_Group_xxx.pdf\n",
    "```\n",
    "> Replace `xxx` in your file names with your **three‑digit** group number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f479a2-7ad4-4321-831c-e9350a3121cc",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Configuration & preflight (group, paths)  \n",
    "2. Database connection\n",
    "3. Reset and create staging schema (`stg2_xxx`) from DDL file \n",
    "4. Load CSVs into stg2_xxx (order-sensitive)  \n",
    "5. Reset and create warehouse (`dwh2_xxx`) from DDL file \n",
    "6. ETL runner (executes `etl/a2_etl*.sql`)  \n",
    "7. SQL queries\n",
    "8. Atoti setup and build the OLAP cube (scaffold)\n",
    "9. Define hierarchies and measures\n",
    "10. MDX queries\n",
    "11. Batch executor: run all .mdx → CSV (+ an index)\n",
    "12. Create database dump\n",
    "13. Submission checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04273c9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration & preflight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07def75d-1bd4-4c44-a341-9e6e71742fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "# XXX = \"001\"               # # three digits, e.g. \"007\"\n",
    "# ...\n",
    "# XXX = \"031\"               # # three digits, e.g. \"007\"\n",
    "# ...\n",
    "# XXX = \"071\"               # # three digits, e.g. \"007\"\n",
    "# ...\n",
    "# XXX = \"199\"               # # three digits, e.g. \"007\"\n",
    "XXX = \"080\"               # # three digits, e.g. \"007\"\n",
    "\n",
    "VERBOSE_SQL = False             # print progress when running .sql files\n",
    "LOAD_ORDER_CSV = []             # or fill later\n",
    "LOAD_ORDER_DATA = []            # or fill later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ae5428-70cc-4237-b3ff-c228686b3271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Atoti 0.9.9!\n",
      "\n",
      "By using this community edition, you agree with the license available at https://docs.activeviam.com/products/atoti/python-sdk/latest/eula.html.\n",
      "Browse the official documentation at https://docs.activeviam.com/products/atoti/python-sdk.\n",
      "Join the community at https://www.atoti.io/register.\n",
      "\n",
      "Atoti collects telemetry data, which is used to help understand how to improve the product.\n",
      "If you don't wish to send usage data, you can request a trial license at https://www.atoti.io/evaluation-license-request.\n",
      "\n",
      "You can hide this message by setting the `ATOTI_HIDE_EULA_MESSAGE` environment variable to True.\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "import shutil, subprocess, os\n",
    "import json, hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from urllib.parse import quote_plus\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import sqlparse\n",
    "from sqlalchemy import create_engine, text, engine\n",
    "\n",
    "import atoti as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6d15b5-6806-4dd0-bb36-5e3534585b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: atoti\n",
      "Version: 0.9.9\n",
      "Summary: Explore metrics across hundreds of dimensions, analyze live data at its most granular level and perform what-if simulations at unparalleled speed\n",
      "Home-page: https://www.atoti.io\n",
      "Author: \n",
      "Author-email: ActiveViam <dev@atoti.io>\n",
      "License: \n",
      "Location: /opt/miniconda3/envs/dwh/lib/python3.12/site-packages\n",
      "Requires: atoti-client, atoti-server, jdk4py\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show atoti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004841f2-fb79-4e7f-88f3-9eed76c28bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/csv\n",
      "DDL dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/ddl\n",
      "ETL dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/etl\n",
      "MDX dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/mdx\n",
      "MDX_out dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/mdx_out\n",
      "SQL dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/sql\n",
      "SQLdump dir: /Users/philippmoessner/Desktop/Business Intelligence/tuwien_bi/DWH2_080/sqldump\n"
     ]
    }
   ],
   "source": [
    "# === Toggles & paths ===\n",
    "root_dir = Path.cwd()\n",
    "csv_dir = root_dir / \"csv\"\n",
    "ddl_dir = root_dir / \"ddl\"\n",
    "etl_dir = root_dir / \"etl\"\n",
    "mdx_dir = root_dir / \"mdx\"\n",
    "mdx_out_dir = root_dir / \"mdx_out\"\n",
    "sql_dir = root_dir / \"sql\"\n",
    "sqldump_dir = root_dir / \"sqldump\"\n",
    "\n",
    "SCHEMA_STG = f\"stg2_{XXX}\"\n",
    "SCHEMA_DWH = f\"dwh2_{XXX}\"\n",
    "\n",
    "# files we expect in the ddl subfolder\n",
    "STG2_RESET  = ddl_dir / f\"airq_reset_stg2_{XXX}.sql\"\n",
    "STG2_CREATE = ddl_dir / f\"airq_create_stg2_{XXX}.sql\"\n",
    "DWH2_RESET  = ddl_dir / f\"airq_reset_dwh2_{XXX}.sql\"\n",
    "DWH2_CREATE = ddl_dir / f\"airq_create_dwh2_{XXX}.sql\"\n",
    "\n",
    "print(\"CSV dir:\", csv_dir)\n",
    "print(\"DDL dir:\", ddl_dir)\n",
    "print(\"ETL dir:\", etl_dir)\n",
    "print(\"MDX dir:\", mdx_dir)\n",
    "print(\"MDX_out dir:\", mdx_out_dir)\n",
    "print(\"SQL dir:\", sql_dir)\n",
    "print(\"SQLdump dir:\", sqldump_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5448ef2-8227-465c-a00c-6883b8535a3e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Make database connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8816fcb6-7038-486e-ae99-2efc346eeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting via: postgresql+psycopg2://\\1:***@localhost:5432/airq\n",
      "SET ROLE grp_080 ✓\n",
      "current_user: grp_080\n"
     ]
    }
   ],
   "source": [
    "# === Minimal config & connect ===\n",
    "DB_USER = f\"grp_{XXX}\"\n",
    "DB_NAME = \"airq\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# a password is asked once per run; enter empty password if your local pg_hba allows trust/peer\n",
    "pw = getpass(f\"Password for {DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME} (leave empty if not needed): \")\n",
    "DSN = f\"postgresql+psycopg2://{DB_USER}:{quote_plus(pw)}@{DB_HOST}:{DB_PORT}/{DB_NAME}\" if pw \\\n",
    "      else f\"postgresql+psycopg2://{DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "def _mask_dsn(dsn: str) -> str:\n",
    "    try:\n",
    "        return str(engine.make_url(dsn).set(password=\"***\"))\n",
    "    except Exception:\n",
    "        return re.sub(r\"://([^:@]+)(?::[^@]*)?@\", r\"://\\\\1:***@\", dsn)\n",
    "\n",
    "engine = create_engine(DSN, future=True, pool_pre_ping=True)\n",
    "print(\"Connecting via:\", _mask_dsn(DSN))\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    # best-effort: set the role if it exists; don't crash if not\n",
    "    try:\n",
    "        conn.exec_driver_sql(f\"SET ROLE grp_{XXX}\")\n",
    "        print(f\"SET ROLE grp_{XXX} ✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"(no SET ROLE: {e.__class__.__name__})\")\n",
    "    who = conn.exec_driver_sql(\"select current_user\").scalar_one()\n",
    "    print(\"current_user:\", who)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a704b837-f6a0-4c73-b2a5-e3f069aa6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sqlscript(\n",
    "    path: str,\n",
    "    *,\n",
    "    engine,\n",
    "    progress: bool = True,      # progress/verbosity- show progress OR keep output quiet\n",
    "    add_search_path: bool = False,\n",
    "    schema_dwh: str | None = None,\n",
    "    schema_stg: str | None = None,\n",
    "    title: str | None = None,      # optional title\n",
    "    strip_psql_meta: bool = True,  # psql meta stripping\n",
    "):\n",
    "    \"\"\"\n",
    "    Execute all statements in a .sql file.\n",
    "    - Returns the LAST result set as a pandas.DataFrame if any statement returns rows; else None.\n",
    "    - Set progress=False to suppress progress/header prints (great for check scripts).\n",
    "    \"\"\"\n",
    "\n",
    "    raw = Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Strip psql meta-commands (e.g., \\i, \\set) if requested\n",
    "    if strip_psql_meta:\n",
    "        raw = \"\\n\".join(\n",
    "            line for line in raw.splitlines()\n",
    "            if not line.lstrip().startswith(\"\\\\\")\n",
    "        )\n",
    "\n",
    "    # Optional search_path prologue\n",
    "    prologue = \"\"\n",
    "    if add_search_path:\n",
    "        schs = [s for s in (schema_dwh, schema_stg) if s]\n",
    "        if schs:\n",
    "            prologue = f\"SET search_path TO {', '.join(schs)};\\n\"\n",
    "\n",
    "    script = prologue + raw\n",
    "    stmts = [s.strip() for s in sqlparse.split(script) if s and s.strip(\" ;\\n\\t\")]\n",
    "\n",
    "    if progress:\n",
    "        hdr = f\"▶ {title}\" if title else \"▶ Running SQL script\"\n",
    "        print(f\"{hdr}: {path} ({len(stmts)} statements)\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    last_df = None\n",
    "    with engine.begin() as conn:\n",
    "        for i, stmt in enumerate(stmts, start=1):\n",
    "            if not stmt:\n",
    "                continue\n",
    "            start = time.time()\n",
    "            try:\n",
    "                if progress:\n",
    "                    preview = \" \".join(stmt.split())[:120]\n",
    "                    print(f\"  {i:>3}: {preview} ...\")\n",
    "\n",
    "                cursor = conn.exec_driver_sql(stmt)\n",
    "\n",
    "                if cursor.returns_rows:\n",
    "                    rows = cursor.fetchall()\n",
    "                    cols = cursor.keys()\n",
    "                    last_df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "                if progress:\n",
    "                    print(f\"       OK ({time.time() - start:.3f}s)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Raise with a helpful preview even when progress=False\n",
    "                preview = \" \".join(stmt.split())[:160]\n",
    "                raise RuntimeError(\n",
    "                    f\"SQL error in statement #{i}: {preview}\"\n",
    "                ) from e\n",
    "\n",
    "    if progress:\n",
    "        print(f\"✅ Done in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    return last_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6e6b6-2aa7-43b6-8477-41f4b15694b0",
   "metadata": {},
   "source": [
    "## 3) Reset and create **staging schema** (`stg2_xxx`) from DDL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d04d190-0479-42fb-babe-9d782e26235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== STAGING-ONLY RESET: stg2_080 ==\n"
     ]
    }
   ],
   "source": [
    "print(f\"== STAGING-ONLY RESET: stg2_{XXX} ==\")\n",
    "try:\n",
    "    for p in (STG2_RESET, STG2_CREATE):\n",
    "        run_sqlscript(p, engine=engine, progress=VERBOSE_SQL)\n",
    "except Exception as e:\n",
    "    print(f\"!! Reset & create failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc162a",
   "metadata": {},
   "source": [
    "## 4) Load CSV → `stg2_xxx` with Pandas `.to_sql()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855c8fcd-8591-40fb-b435-58d118e7e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folder_to_stg(\n",
    "    folder_name: str,\n",
    "    engine,\n",
    "    SCHEMA_STG: str,\n",
    "    load_order=None,\n",
    "    if_exists: str = \"append\",\n",
    "    chunksize: int = 20000,\n",
    "):\n",
    "    global root_dir  # expected to be defined earlier\n",
    "    src_dir = Path(root_dir) / folder_name\n",
    "    if not src_dir.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {src_dir}\")\n",
    "\n",
    "    def load_one(name: str):\n",
    "        path = src_dir / f\"{name}.csv\"\n",
    "        if not path.exists():\n",
    "            print(\"Missing CSV:\", path.name)\n",
    "            return 0\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            na_values=[\"\\\\N\"],\n",
    "            keep_default_na=False,\n",
    "            low_memory=False,\n",
    "        )\n",
    "        # Convert any *...from / ...to / ...at* to DATE\n",
    "        for col in df.columns:\n",
    "            col_l = col.lower()\n",
    "            if col_l.endswith((\"from\", \"to\", \"at\")):\n",
    "                df[col] = pd.to_datetime(df[col], format=\"%Y-%m-%d\", errors=\"coerce\").dt.date\n",
    "        # Write\n",
    "        df.to_sql(\n",
    "            name,\n",
    "            con=engine,\n",
    "            schema=SCHEMA_STG,\n",
    "            if_exists=if_exists,\n",
    "            index=False,\n",
    "            method=\"multi\",\n",
    "            chunksize=chunksize,\n",
    "        )\n",
    "        print(f\"Loaded {len(df):,} rows → {SCHEMA_STG}.{name}\")\n",
    "        return len(df)\n",
    "\n",
    "    if not load_order:\n",
    "        discovered = sorted([p.stem for p in src_dir.glob(\"*.csv\")])\n",
    "        print(\"No order set yet. CSVs found:\", discovered)\n",
    "        return\n",
    "\n",
    "    t0 = time.time()\n",
    "    total = 0\n",
    "    for name in load_order:\n",
    "        total += load_one(name)\n",
    "    print(f\"⏱️ Total load time: {time.time() - t0:.2f} seconds · {total:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a109ccca-903b-4980-87ff-5092004f0d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 rows → stg2_080.tb_servicetype\n",
      "Loaded 16 rows → stg2_080.tb_role\n",
      "Loaded 484 rows → stg2_080.tb_employee\n",
      "Loaded 20 rows → stg2_080.tb_country\n",
      "Loaded 36 rows → stg2_080.tb_city\n",
      "Loaded 8 rows → stg2_080.tb_readingmode\n",
      "Loaded 4 rows → stg2_080.tb_alert\n",
      "Loaded 30 rows → stg2_080.tb_param\n",
      "Loaded 120 rows → stg2_080.tb_paramalert\n",
      "Loaded 12 rows → stg2_080.tb_sensortype\n",
      "Loaded 115 rows → stg2_080.tb_paramsensortype\n",
      "Loaded 627 rows → stg2_080.tb_sensordevice\n",
      "Loaded 26,316 rows → stg2_080.tb_weather\n",
      "Loaded 22,720 rows → stg2_080.tb_serviceevent\n",
      "Loaded 985,573 rows → stg2_080.tb_readingevent\n",
      "⏱️ Total load time: 94.53 seconds · 1,036,105 rows\n"
     ]
    }
   ],
   "source": [
    "# Loading of original 15 CSV files in the correct order\n",
    "LOAD_ORDER_CSV = [\"tb_servicetype\",\"tb_role\",\"tb_employee\",\"tb_country\",\"tb_city\",\"tb_readingmode\",\n",
    "                  \"tb_alert\",\"tb_param\",\"tb_paramalert\",\"tb_sensortype\",\"tb_paramsensortype\",\"tb_sensordevice\",\n",
    "                  \"tb_weather\",\"tb_serviceevent\",\"tb_readingevent\"]\n",
    "\n",
    "load_folder_to_stg(\"csv\", engine, SCHEMA_STG, load_order=LOAD_ORDER_CSV,  if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bd301",
   "metadata": {},
   "source": [
    "## 5) Reset and create **warehouse** (`dwh2_xxx`) from DDL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56719c08-77d7-45c0-8a04-5b1fe82e988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== DWH-ONLY RESET: dwh2_080 ==\n"
     ]
    }
   ],
   "source": [
    "print(f\"== DWH-ONLY RESET: dwh2_{XXX} ==\")\n",
    "try:\n",
    "    for p in (DWH2_RESET, DWH2_CREATE):\n",
    "        run_sqlscript(p, engine=engine, progress=VERBOSE_SQL)\n",
    "except Exception as e:\n",
    "    print(f\"!! Reset & create failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1af517",
   "metadata": {},
   "source": [
    "\n",
    "## 6) SQL-first ETL — run all files in etl/\n",
    "\n",
    "We execute **all** files matching `etl/a2_etl*.sql` in lexicographic order. Every ETL file must begin with `SET search_path TO dwh2_xxx, stg2_xxx;`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518eb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = sorted(etl_dir.glob(\"a2_etl*.sql\"))\n",
    "if not steps:\n",
    "    print(\"No ETL step files found in etl/ (expected a2_etl*.sql).\")\n",
    "else:\n",
    "    for s in steps:\n",
    "        run_sqlscript(s, engine=engine, progress=VERBOSE_SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a7797-658f-4534-8033-e4b22cfb550b",
   "metadata": {},
   "source": [
    "## 7) SQL-queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037ff92-755f-4706-a90b-13b67a8b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business question Q31 (example)\n",
    "df = run_sqlscript(\"sql/a2_q31.sql\", engine=engine, progress=VERBOSE_SQL)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7903d8-3388-48b7-ba86-1dd14ca23e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business question Q32 (example)\n",
    "df = run_sqlscript(\"sql/a2_q32.sql\", engine=engine, progress=VERBOSE_SQL)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da472e-7ace-4e0f-8b67-c3a49e01919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business question Q33 (example)\n",
    "df = run_sqlscript(\"sql/a2_q33.sql\", engine=engine, progress=VERBOSE_SQL)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3d215-25e7-4eaa-a65b-b4da9fae7522",
   "metadata": {},
   "source": [
    "## 8) Atoti setup and build cube (scaffold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c035ecf-7dee-4ab5-a5db-811fa8d4dd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:63976'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.pop(\"JAVA_HOME\", None)  # let Atoti use its own JDK via jdk4py\n",
    "\n",
    "# Start a new Atoti session\n",
    "session = tt.Session.start()\n",
    "\n",
    "# URL to the Atoti web app\n",
    "session.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1eccaf1-788f-4dce-9df9-1d90def43e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_table(session, name, df, *, keys=None, defaults=None, dtypes=None):\n",
    "    if name in session.tables.keys():\n",
    "        t = session.tables[name]\n",
    "        t.drop()  # delete all rows, keep schema\n",
    "        if defaults:  # non-nullability even for existing tables\n",
    "            for col, val in defaults.items():\n",
    "                t[col].default_value = val  # set after creation too\n",
    "        t.load(df)\n",
    "    else:\n",
    "        t = session.read_pandas(\n",
    "            df,\n",
    "            table_name=name,\n",
    "            keys=keys or (),\n",
    "            default_values=defaults or {},   # set at creation time\n",
    "            data_types=dtypes or {},\n",
    "        )\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fcd46d3-717b-4c56-961f-1126e424a691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Dimensions": {},
       "Measures": {
        "contributors.COUNT": {
         "formatter": "INT[#,###]"
        }
       }
      },
      "text/html": [
       "<ul>\n",
       "<li>AirQ Cube\n",
       "  <ul>\n",
       "  <li>Dimensions\n",
       "    <ul>\n",
       "    </ul>\n",
       "  </li>\n",
       "  <li>Measures\n",
       "    <ul>\n",
       "    <li>contributors.COUNT\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  </ul>\n",
       "</li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<atoti.cube.Cube at 0x308ad1e80>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "AirQ Cube"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load star-schema tables to DataFrames\n",
    "df_time   = pd.read_sql(f\"SELECT * FROM dwh2_{XXX}.dim_timemonth\", engine)\n",
    "df_city   = pd.read_sql(f\"SELECT * FROM dwh2_{XXX}.dim_city\", engine)\n",
    "df_param  = pd.read_sql(f\"SELECT * FROM dwh2_{XXX}.dim_param\", engine)\n",
    "df_alert  = pd.read_sql(f\"SELECT * FROM dwh2_{XXX}.dim_alertpeak\", engine)\n",
    "df_fact   = pd.read_sql(f\"SELECT * FROM dwh2_{XXX}.ft_param_city_month\", engine)\n",
    "\n",
    "time_store  = upsert_table(session, \"dim_timemonth\", df_time,\n",
    "                           keys=[\"month_key\"],\n",
    "                           defaults={\"year_num\": 0, \"quarter_num\": 0, \"month_name\": \"Unknown\"},\n",
    "                           dtypes={\"year_num\": \"int\", \"quarter_num\": \"int\"})\n",
    "\n",
    "city_store  = upsert_table(session, \"dim_city\", df_city,\n",
    "                           keys=[\"city_key\"],\n",
    "                           defaults={\"region_name\": \"Unknown\", \"country_name\": \"Unknown\", \"city_name\": \"Unknown\"})\n",
    "\n",
    "param_store = upsert_table(session, \"dim_param\", df_param,\n",
    "                           keys=[\"param_key\"],\n",
    "                           defaults={\"purpose\": \"Unknown\", \"category\": \"Unknown\", \"param_name\": \"Unknown\"})\n",
    "\n",
    "ap_store    = upsert_table(session, \"dim_alertpeak\", df_alert,\n",
    "                           keys=[\"alertpeak_key\"],\n",
    "                           defaults={\"alert_level_name\": \"None\"})\n",
    "\n",
    "fact_store = upsert_table(session, \"ft_param_city_month\", df_fact, \n",
    "                          keys=[\"ft_pcm_key\"], \n",
    "                          defaults={\"month_key\": 0, \"city_key\": 0, \"param_key\": 0, \"alertpeak_key\": 1000,  # FKs\n",
    "                                    \"reading_events_count\": 0, \"devices_reporting_count\": 0, \n",
    "                                    \"data_volume_kb_sum\": 0, \"recordedvalue_avg\": 0.0, \"recordedvalue_p95\": 0.0, \n",
    "                                    \"exceed_days_any\": 0, \"data_quality_avg\": 0.0, \"missing_days\": 0, \n",
    "                                   }, \n",
    "                          dtypes={\"month_key\": \"int\", \"city_key\": \"int\", \n",
    "                                  \"param_key\": \"int\", \"alertpeak_key\": \"int\", \n",
    "                                  \"reading_events_count\": \"int\", \"devices_reporting_count\": \"int\", \n",
    "                                  \"data_volume_kb_sum\": \"int\", \"recordedvalue_avg\": \"float\", \n",
    "                                  \"recordedvalue_p95\": \"float\", \"exceed_days_any\": \"int\", \n",
    "                                  \"data_quality_avg\": \"float\", \"missing_days\": \"int\", \n",
    "                                 },\n",
    "                         )\n",
    "\n",
    "# Define joins once per fresh session - can re-run the cell without redefining joins\n",
    "if not getattr(session, \"_airq_joins_done\", False):\n",
    "    fact_store.join(time_store,   fact_store[\"month_key\"]     == time_store[\"month_key\"])\n",
    "    fact_store.join(city_store,   fact_store[\"city_key\"]      == city_store[\"city_key\"])\n",
    "    fact_store.join(param_store,  fact_store[\"param_key\"]     == param_store[\"param_key\"])\n",
    "    fact_store.join(ap_store,     fact_store[\"alertpeak_key\"] == ap_store[\"alertpeak_key\"])\n",
    "    session._airq_joins_done = True\n",
    "\n",
    "# Create or reuse the cube\n",
    "cube_name = \"AirQ Cube\"\n",
    "cube = (\n",
    "    session.cubes[cube_name]\n",
    "    if cube_name in session.cubes.keys()\n",
    "    else session.create_cube(fact_store, cube_name, mode=\"manual\")\n",
    ")\n",
    "\n",
    "# Access cube components\n",
    "m, h, l = cube.measures, cube.hierarchies, cube.levels\n",
    "\n",
    "cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ba4ba-8c1b-410c-bbe2-b8f121fb2e20",
   "metadata": {},
   "source": [
    "## 9) Define hierarchies and measures\n",
    "Define explicit hierarchies in Atoti:\n",
    "\n",
    "1) Time: Year → Quarter → Month,\n",
    "2) Geo: Region → Country → City,\n",
    "3) Param: Purpose → Category → Param,\n",
    "4) Alert: Level (sorted by rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4429ac35-8624-46de-8e9d-9b09f4c80d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define hierarchies\n",
    "h[\"Time\"] = [\n",
    "    time_store[\"year_num\"],\n",
    "    time_store[\"quarter_num\"],\n",
    "    time_store[\"month_name\"],\n",
    "]\n",
    "h[\"Geo\"] = [\n",
    "    city_store[\"region_name\"],\n",
    "    city_store[\"country_name\"],\n",
    "    city_store[\"city_name\"],\n",
    "]\n",
    "h[\"Param\"] = [\n",
    "    param_store[\"purpose\"],\n",
    "    param_store[\"category\"],\n",
    "    param_store[\"param_name\"],\n",
    "]\n",
    "h[\"Alert\"] = [\n",
    "    ap_store[\"alert_level_name\"],\n",
    "]\n",
    "\n",
    "# TODO: define measures\n",
    "m[\"Reading Events\"] = tt.agg.sum(fact_store[\"reading_events_count\"])        # fully additive\n",
    "m[\"Devices Reporting\"] = tt.agg.sum(fact_store[\"devices_reporting_count\"])  # fully additive\n",
    "m[\"Data Volume (KB)\"] = tt.agg.sum(fact_store[\"data_volume_kb_sum\"])        # fully additive\n",
    "m[\"Missing Days\"] = tt.agg.sum(fact_store[\"missing_days\"])                  # fully additive\n",
    "m[\"Exceed Days (any)\"] = tt.agg.sum(fact_store[\"exceed_days_any\"])          # fully additive\n",
    "m[\"Avg Recorded Value\"] = tt.agg.mean(fact_store[\"recordedvalue_avg\"])      # semi-additive\n",
    "m[\"P95 Recorded Value\"] = tt.agg.mean(fact_store[\"recordedvalue_p95\"])      # semi-additive\n",
    "m[\"Avg Data Quality\"] = tt.agg.mean(fact_store[\"data_quality_avg\"])         # semi-additive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3404b2db-3869-4fb6-8255-1099bf3919ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order months as in calendar, not alphabetically\n",
    "month_lvl = cube.hierarchies[\"Time\"][\"month_name\"]\n",
    "month_lvl.order = tt.CustomOrder(first_elements=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
    "                                                 \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75861cbf-206f-4b40-88da-51818ea0dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order alert levels from least to most harmful\n",
    "alert_lvl = cube.hierarchies[\"Alert\"][\"alert_level_name\"]\n",
    "alert_lvl.order = tt.CustomOrder(first_elements=[\"None\", \"Yellow\", \"Orange\", \"Red\", \"Crimson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "705f05c8-417c-4884-b04b-7b3e585de57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Dimensions": {
        "dim_alertpeak": {
         "Alert": [
          "alert_level_name"
         ]
        },
        "dim_city": {
         "Geo": [
          "region_name",
          "country_name",
          "city_name"
         ]
        },
        "dim_param": {
         "Param": [
          "purpose",
          "category",
          "param_name"
         ]
        },
        "dim_timemonth": {
         "Time": [
          "year_num",
          "quarter_num",
          "month_name"
         ]
        }
       },
       "Measures": {
        "Avg Data Quality": {
         "formatter": "DOUBLE[#,###.00]"
        },
        "Avg Recorded Value": {
         "formatter": "DOUBLE[#,###.00]"
        },
        "Data Volume (KB)": {
         "formatter": "INT[#,###]"
        },
        "Devices Reporting": {
         "formatter": "INT[#,###]"
        },
        "Exceed Days (any)": {
         "formatter": "INT[#,###]"
        },
        "Missing Days": {
         "formatter": "INT[#,###]"
        },
        "P95 Recorded Value": {
         "formatter": "DOUBLE[#,###.00]"
        },
        "Reading Events": {
         "formatter": "INT[#,###]"
        },
        "contributors.COUNT": {
         "formatter": "INT[#,###]"
        }
       }
      },
      "text/html": [
       "<ul>\n",
       "<li>AirQ Cube\n",
       "  <ul>\n",
       "  <li>Dimensions\n",
       "    <ul>\n",
       "    <li>dim_alertpeak\n",
       "      <ul>\n",
       "      <li>Alert\n",
       "        <ol>\n",
       "        <li>alert_level_name</li>\n",
       "        </ol>      </li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>dim_city\n",
       "      <ul>\n",
       "      <li>Geo\n",
       "        <ol>\n",
       "        <li>region_name</li>\n",
       "        <li>country_name</li>\n",
       "        <li>city_name</li>\n",
       "        </ol>      </li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>dim_param\n",
       "      <ul>\n",
       "      <li>Param\n",
       "        <ol>\n",
       "        <li>purpose</li>\n",
       "        <li>category</li>\n",
       "        <li>param_name</li>\n",
       "        </ol>      </li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>dim_timemonth\n",
       "      <ul>\n",
       "      <li>Time\n",
       "        <ol>\n",
       "        <li>year_num</li>\n",
       "        <li>quarter_num</li>\n",
       "        <li>month_name</li>\n",
       "        </ol>      </li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  <li>Measures\n",
       "    <ul>\n",
       "    <li>Avg Data Quality\n",
       "      <ul>\n",
       "      <li>formatter: DOUBLE[#,###.00]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>Avg Recorded Value\n",
       "      <ul>\n",
       "      <li>formatter: DOUBLE[#,###.00]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>Data Volume (KB)\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>Devices Reporting\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>Exceed Days (any)\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>Missing Days\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>P95 Recorded Value\n",
       "      <ul>\n",
       "      <li>formatter: DOUBLE[#,###.00]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>Reading Events\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    <li>contributors.COUNT\n",
       "      <ul>\n",
       "      <li>formatter: INT[#,###]</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  </ul>\n",
       "</li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<atoti.cube.Cube at 0x308ad1e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "AirQ Cube"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b656716-9e3e-44d5-9fe5-71db898afff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hierarchies and their levels:\n",
      " - ('dim_timemonth', 'Time') → levels: ['year_num', 'quarter_num', 'month_name']\n",
      " - ('dim_param', 'Param') → levels: ['purpose', 'category', 'param_name']\n",
      " - ('dim_alertpeak', 'Alert') → levels: ['alert_level_name']\n",
      " - ('dim_city', 'Geo') → levels: ['region_name', 'country_name', 'city_name']\n",
      "\\Measures:\n",
      "  - Avg Data Quality\n",
      "  - P95 Recorded Value\n",
      "  - Devices Reporting\n",
      "  - Avg Recorded Value\n",
      "  - contributors.COUNT\n",
      "  - Exceed Days (any)\n",
      "  - Reading Events\n",
      "  - Data Volume (KB)\n",
      "  - update.TIMESTAMP\n",
      "  - Missing Days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "/var/folders/3_/4191z6q51251z9ggjkycrb5m0000gn/T/ipykernel_21469/4001172213.py:6: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  print(\"\\Measures:\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nHierarchies and their levels:\")\n",
    "for h_name, hierarchy in cube.hierarchies.items():\n",
    "    level_names = [getattr(level, \"name\", str(level)) for level in hierarchy]\n",
    "    print(f\" - {h_name} → levels: {level_names}\")\n",
    "\n",
    "print(\"\\Measures:\")\n",
    "for m in cube.measures.keys():\n",
    "    print(\"  -\", m)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df35aab-0b9a-4fa0-94b0-060691c857b0",
   "metadata": {},
   "source": [
    "## 10) MDX queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b1636-46b7-4a42-95f9-ca33301c1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDX cell magic: let us write MDX code like this:\n",
    "#   %%mdx\n",
    "#   SELECT ... FROM [AirQ Cube]\n",
    "#\n",
    "# Requirements: a live `session` from atoti and the cube already created.\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import display\n",
    "\n",
    "@register_cell_magic\n",
    "def mdx(line, cell):\n",
    "    \"\"\"Run MDX in this cell and display a DataFrame.\n",
    "    Usage:\n",
    "        %%mdx\n",
    "        SELECT ...\n",
    "        FROM [AirQ Cube]\n",
    "    \"\"\"\n",
    "    q = cell.strip()\n",
    "    df = session.query_mdx(q)   # Atoti returns levels on index, measures as columns\n",
    "    return df                   # df = _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6629fe-6a93-406a-94c9-aa7d15915795",
   "metadata": {},
   "source": [
    "### 10.1) Business question Q31 (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d45b0-079d-425a-aad4-e70adf0d35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mdx\n",
    "\n",
    "-- 31. For parameter O3, list the Top 10 Cities by P95 Recorded Value for 2023.\n",
    "-- Return the 10 cities with the highest values on rows (highest → lowest) and one column with P95 Recorded Value for 2023.\n",
    "SELECT\n",
    "  { [Measures].[P95 Recorded Value] } ON COLUMNS,\n",
    "  TOPCOUNT(\n",
    "    NONEMPTY(\n",
    "      [dim_city].[Geo].[city_name].Members,\n",
    "      [Measures].[P95 Recorded Value]\n",
    "    ),\n",
    "    10, [Measures].[P95 Recorded Value]\n",
    "  ) ON ROWS\n",
    "FROM [AirQ Cube]\n",
    "WHERE (\n",
    "  [dim_timemonth].[Time].[year_num].&[2023],\n",
    "  [dim_param].[Param].[param_name].&[O3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611f950-2599-4c20-8b03-53bd2b054363",
   "metadata": {},
   "source": [
    "### 10.2) Business question Q32 (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65791ff-62c7-46a4-89eb-a3e22d8fe87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mdx \n",
    "\n",
    "-- 32. For 2024, show Data Volume (KB) by City for category ‘Volatile Organic Compound’, and list the Top 10 cities.\n",
    "-- Return the Top 10 cities on rows (highest -> lowest) and one column with Data Volume (KB) for 2024, limited to the Volatile Organic Compound category.\n",
    "SELECT\n",
    "  { [Measures].[Data Volume (KB)] } ON COLUMNS,\n",
    "  TOPCOUNT(\n",
    "    NONEMPTY([dim_city].[Geo].[city_name].Members, [Measures].[Data Volume (KB)]),\n",
    "    10, [Measures].[Data Volume (KB)]\n",
    "  ) ON ROWS\n",
    "FROM (\n",
    "  SELECT ( [dim_timemonth].[Time].[year_num].&[2024] ) ON 0 FROM (\n",
    "    SELECT (\n",
    "      FILTER(\n",
    "        [dim_param].[Param].[param_name].Members,\n",
    "        ANCESTOR(\n",
    "          [dim_param].[Param].CurrentMember,\n",
    "          [dim_param].[Param].[category]\n",
    "        ).Name = \"Volatile Organic Compound\"\n",
    "      )\n",
    "    ) ON 0 FROM [AirQ Cube]\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d41b7c-e13e-4ef3-85fd-99adbca247ac",
   "metadata": {},
   "source": [
    "### 10.3) Business question Q33 (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b80e2-8ee8-42a4-9048-c2a81fe0155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mdx\n",
    "\n",
    "-- 33. For parameter PM4 in 2024, return for each Country the Month with the highest Avg Data Quality.\n",
    "-- Return one row per Country × Month (the month with the highest Avg Data Quality in 2024) and one column with Avg Data Quality.\n",
    "SELECT\n",
    "  { [Measures].[Avg Data Quality] } ON COLUMNS,\n",
    "  NON EMPTY\n",
    "    GENERATE(\n",
    "      [dim_city].[Geo].[country_name].Members,\n",
    "      TOPCOUNT(\n",
    "        CROSSJOIN(\n",
    "          { [dim_city].[Geo].CurrentMember },\n",
    "          Descendants(\n",
    "            [dim_timemonth].[Time].[year_num].&[2024],\n",
    "            [dim_timemonth].[Time].[month_name]\n",
    "          )\n",
    "        ),\n",
    "        1, [Measures].[Avg Data Quality]\n",
    "      )\n",
    "    ) ON ROWS\n",
    "FROM [AirQ Cube]\n",
    "WHERE ( [dim_param].[Param].[param_name].&[PM4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3be9e-5681-45b3-aeaf-d0ba10f0ffc0",
   "metadata": {},
   "source": [
    "## 11) Batch executor: run all .mdx → CSV (+ an index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc647-1b25-41eb-904f-e085c8393323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mdx_folder(\n",
    "    mdx_folder=\"mdx\",\n",
    "    out_folder=\"mdx_out\",\n",
    "    pattern=\"*.mdx\",\n",
    "    overwrite=True,\n",
    "    index_csv=\"mdx_index.csv\",\n",
    "):\n",
    "    mdx_path = Path(mdx_folder)\n",
    "    out_path = Path(out_folder)\n",
    "    mdx_path.mkdir(exist_ok=True)\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    records = []\n",
    "    files = sorted(mdx_path.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"No MDX files found in {mdx_path.resolve()}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for f in files:\n",
    "        q = f.read_text(encoding=\"utf-8\")\n",
    "        t0 = time.time()\n",
    "        error = None\n",
    "        rows = cols = 0\n",
    "        dest = out_path / f\"{f.stem}.csv\"\n",
    "\n",
    "        try:\n",
    "            df = session.query_mdx(q).reset_index()\n",
    "            rows, cols = df.shape\n",
    "            if overwrite or not dest.exists():\n",
    "                df.to_csv(dest, index=False)\n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        records.append({\n",
    "            \"file\": f.name,\n",
    "            \"csv\": dest.name,\n",
    "            \"rows\": rows,\n",
    "            \"cols\": cols,\n",
    "            \"seconds\": round(elapsed, 3),\n",
    "            \"error\": error,\n",
    "        })\n",
    "\n",
    "    index_df = pd.DataFrame(records)\n",
    "    index_path = out_path / index_csv\n",
    "    index_df.to_csv(index_path, index=False)\n",
    "    print(f\"Done. Index saved to {index_path}\")\n",
    "    return index_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92a5ed-117d-44e9-8f60-8f687072d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all MDX files:\n",
    "index_df = run_mdx_folder()\n",
    "index_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34595cc9",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Create `sqldump/sqldump_airq_dwh2_xxx.sql`\n",
    "\n",
    "We run `pg_dump -n dwh2_xxx --no-owner --no-privileges` to keep dumps portable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bec06-8be2-423c-8e88-7699563fc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create sqldump/sqldump_airq_dwh2_xx.sql (pg_dump) ===\n",
    "sqldump_dir.mkdir(exist_ok=True)\n",
    "outfile = sqldump_dir / f\"sqldump_airq_dwh2_{XXX}.sql\"\n",
    "\n",
    "pg_dump = shutil.which(\"pg_dump\") or \"pg_dump\"\n",
    "cmd = [\n",
    "    pg_dump,\n",
    "    \"-h\", DB_HOST,\n",
    "    \"-p\", str(DB_PORT),\n",
    "    \"-U\", DB_USER,\n",
    "    \"-d\", DB_NAME,\n",
    "    \"-n\", f\"dwh2_{XXX}\",\n",
    "    \"--no-owner\",\n",
    "    \"--no-privileges\",\n",
    "    \"-f\", str(outfile),\n",
    "]\n",
    "\n",
    "# Avoid echoing the password; supply it via env if provided\n",
    "env = dict(os.environ)\n",
    "if 'pw' in globals() and pw:\n",
    "    env[\"PGPASSWORD\"] = pw\n",
    "\n",
    "print(\"Running:\", \" \".join(cmd).replace(DB_USER, \"<user>\"))\n",
    "try:\n",
    "    subprocess.run(cmd, check=True, env=env)\n",
    "    print(\"✓ Dump created at\", outfile)\n",
    "except Exception as e:\n",
    "    print(\"pg_dump failed; try this manually in a terminal:\\n\", \" \".join(cmd), \"\\nError:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faae96",
   "metadata": {},
   "source": [
    "## 13) Submission checklist (put these in your **ZIP**)\n",
    "\n",
    "- `csv/` — CSV files \n",
    "- `ddl/` — DDL scripts \n",
    "- `etl/` — Your `a2_etl*.sql` files (ETL scripts)\n",
    "- `mdx/` — Your `a2_q{NN}_{A|B}.mdx` files (MDX queries for business questions)\n",
    "- `mdx_out/` — Your `a2_q{NN}_{A|B}.csv` files (results of MDX queries)\n",
    "- `pdf/` — Your `a2_q{NN}.pdf` files (Dashboard exports as .pdf)\n",
    "- `sql/` — Your `a2_q{NN}_{A|B}.sql` files (SQL queries for business questions)\n",
    "- `sqldump/` — `sqldump_airq_dwh2_xxx.sql`  \n",
    "- `AirQ_Part2_xxx.ipynb`\n",
    "- `group_xxx.txt`\n",
    "- `Report_Part2_Group_xxx.pdf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4edfd-4f66-4c98-a8cb-42c10a59290e",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
